{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"spamdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're going to be working with a database of emails, some of which are spam and some are not. Here's a link to the source on UCI Repository: https://archive.ics.uci.edu/ml/datasets/spambase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors\n",
    "\n",
    "The predictive features are as follows: \n",
    "\n",
    "48 continuous real [0,100] attributes of type word_freq_WORD\n",
    "= percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail. A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.\n",
    "\n",
    "6 continuous real [0,100] attributes of type char_freq_CHAR]\n",
    "= percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
    "\n",
    "1 continuous real [1,...] attribute of type capital_run_length_average\n",
    "= average length of uninterrupted sequences of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
    "= length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
    "= sum of length of uninterrupted sequences of capital letters\n",
    "= total number of capital letters in the e-mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target\n",
    "\n",
    "Our target is the classification of spam or not. It's encoded in the final column of the dataset as:\n",
    "\n",
    "1 nominal {0,1} class attribute of type spam\n",
    "= denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the spambase data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get it ready for  model fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_spambase_data(filename):\n",
    "    \"\"\" \n",
    "    Given a filename return X and Y numpy arrays.\n",
    "\n",
    "    X is of size number of rows x num_features.\n",
    "    Y is an array of size the number of rows.\n",
    "    Y is the last element of each row. (Convert 0 to -1)\n",
    "    \"\"\"\n",
    "    dataset = np.loadtxt(filename, delimiter=\",\")\n",
    "    K = len(dataset[0])\n",
    "    Y = dataset[:, K - 1]\n",
    "    X = dataset[:, 0 : K - 1]\n",
    "    Y = np.array([-1. if y == 0. else 1. for y in Y])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.000e+00, 0.000e+00, 0.000e+00, ..., 3.850e+00, 1.900e+01,\n",
       "         7.700e+01],\n",
       "        [9.000e-02, 0.000e+00, 9.000e-02, ..., 3.704e+00, 4.800e+01,\n",
       "         7.260e+02],\n",
       "        [7.800e-01, 0.000e+00, 7.800e-01, ..., 2.555e+00, 2.200e+01,\n",
       "         1.150e+02],\n",
       "        ...,\n",
       "        [3.700e-01, 1.800e-01, 1.800e-01, ..., 3.455e+00, 2.400e+01,\n",
       "         3.870e+02],\n",
       "        [0.000e+00, 0.000e+00, 8.000e-01, ..., 2.360e+00, 3.500e+01,\n",
       "         5.900e+01],\n",
       "        [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.666e+00, 7.000e+00,\n",
       "         4.500e+01]]),\n",
       " array([ 1., -1.,  1., ..., -1., -1., -1.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_spambase_data(\"spamdata/spambase.train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper function for scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, X_val):\n",
    "    \"\"\" Given X, X_val compute X_scaled, X_val_scaled\n",
    "    \n",
    "    return X_scaled, X_val_scaled\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    return X_scaled, X_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = parse_spambase_data(PATH/\"spambase.train\")\n",
    "X_val, Y_val = parse_spambase_data(PATH/\"spambase.test\")\n",
    "X, X_val = normalize(X, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an example Neural Network Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small NNs w/ 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, D, seed, hidden=10):\n",
    "        super(NN, self).__init__()\n",
    "        torch.manual_seed(seed) # this is for reproducibility\n",
    "        self.linear1 = nn.Linear(D, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hidden) #normalizes batches\n",
    "        #forces the hidden layer features to have mean 0 and unit var\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #make an nn with 1 hidden layer\n",
    "        #the hidden layer should be activated with ReLU\n",
    "        #which can be found in F.relu\n",
    "        #you can also batch normalize if you want\n",
    "        \n",
    "        \n",
    "        #...\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(NN(10,1).parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Sketch the NN diagram which corresponds to this class. Would we think of this network as a weak learner or a strong learner? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Boost\n",
    "\n",
    "If we want to do gradient boosting we'll need pseudoresiduals. Recall the most general form of a pseudoresidual is:\n",
    "\n",
    "$$r_i = - \\frac{\\partial L}{\\partial f} \\bigg| _{f = f_{m-1}(x_i)}$$\n",
    "\n",
    "If we use logloss then $L(y, f) = log(1+e^{-yf}).$\n",
    "\n",
    "### Calculate pseudoresidual for classification for $m^{th}$ stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pseudo_residual(y, fm):\n",
    "    \"\"\" \n",
    "    vectorized computation of the pseudoresidual for logloss\n",
    "    \"\"\"\n",
    "    res = y/(1+np.exp(y*fm))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function which fits a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitNN(model, X, r, epochs=20, lr=0.1, verbose = False):\n",
    "    \"\"\" \n",
    "    Fit a regression model to the pseudo-residuals.\n",
    "        \n",
    "    Returns the fitted values on training data as a numpy array.\n",
    "    Shape of the return should be (N,) not (N,1).\n",
    "    \"\"\"\n",
    "    \n",
    "    #get data as float tensors w proper dimensions\n",
    "    x = torch.FloatTensor(X)\n",
    "    y = torch.FloatTensor(r).unsqueeze(1)\n",
    "    \n",
    "    #specify Adam as optimizer\n",
    "    #could add weight_decay if we want to attenuate lr over time\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train() #training mode\n",
    "        out = model(x) #pred f_{m-1}(x_i)\n",
    "        \n",
    "        \n",
    "        #calculate empirical log loss \n",
    "        #...\n",
    "        \n",
    "        #grad descent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"loss {:.3f}\".format(loss.item()))\n",
    "        out = out.view(-1)\n",
    "        \n",
    "    return out.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 57)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D = X.shape\n",
    "N, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (linear1): Linear(in_features=57, out_features=10, bias=True)\n",
       "  (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (bn1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN(D = D, seed = 1)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Boosting: First stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4019940876152855"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mean = Y.mean()\n",
    "f0 = np.log((1 + y_mean)/(1 - y_mean))\n",
    "f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = Y/(1 + np.exp(Y*f0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat1 = fitNN(model, X, r1, epochs=20, lr=0.1)\n",
    "f1 = f0 + nu * yhat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then ...\n",
    "\n",
    "r2 = Y/(1 + np.exp(Y*f1))\n",
    "\n",
    "#...and so on until m = M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function which does the boosting automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to do the boosting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostingNN(X, Y, num_iter, nu):\n",
    "    \"\"\"Given an numpy matrix X, an array y,\n",
    "    and the number of iterations num_iter (aka M),\n",
    "    return the fitted trees and weights \n",
    "   \n",
    "    Input: X, y, num_iter\n",
    "    Outputs: array of Regression models\n",
    "    Assumes y is in {-1, 1}\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    N, D = X.shape\n",
    "    seeds = [s+1 for s in range(num_iter)] # use these seeds to call the models\n",
    "\n",
    "    #boost the NNs\n",
    "    y_mean = Y.mean()\n",
    "    f0 = np.log((1 + y_mean)/(1 - y_mean))\n",
    "    fm = f0\n",
    "    for t in range(num_iter):\n",
    "        res = Y/(1+np.exp(Y*fm))\n",
    "        \n",
    "        \n",
    "        #call the model with the t^th seed\n",
    "        #...\n",
    "        \n",
    "        #fit it\n",
    "        #...\n",
    "        \n",
    "        #additive stagewise modeling\n",
    "        #...\n",
    "        \n",
    "        models.append(model)\n",
    "    return f0, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_predict(X, f0, models, nu):\n",
    "    \"\"\"Given X, models, f0 and nu predict y_hat in {-1, 1}\n",
    "    \n",
    "    y_hat should be a numpy array with shape (N,)\n",
    "    \"\"\"\n",
    "    y_hat = f0\n",
    "    x = torch.FloatTensor(X)\n",
    "    for model in models:\n",
    "        model.eval() #eval mode\n",
    "        out = model(x).view(-1)\n",
    "        y_hat += nu*out.detach().numpy()\n",
    "\n",
    "    y_hat = 2 * (y_hat > 0) - 1 #this is effectively sign(y_hat)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing & Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, pred):\n",
    "    return np.sum(y == pred) / float(y.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = parse_spambase_data(PATH/\"tiny.spam.train\")\n",
    "X_val, Y_val = parse_spambase_data(PATH/\"tiny.spam.test\")\n",
    "X, X_val = normalize(X, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.around(X_val[0, :3],3)\n",
    "assert(np.array_equal(xx, np.array([-0.433, -0.491, -0.947])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = parse_spambase_data(PATH/\"spambase.train\")\n",
    "X_val, Y_val = parse_spambase_data(PATH/\"spambase.test\")\n",
    "X, X_val = normalize(X, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.around(X[0, :3],3)\n",
    "assert(np.array_equal(xx, np.array([-0.343, -0.168, -0.556])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([-1, -1, 1, 1])\n",
    "fm = np.array([-0.4, .1, -0.3 , 2])\n",
    "res = compute_pseudo_residual(y, fm)\n",
    "xx = np.around(res, 3)\n",
    "actual = np.array([-0.401, -0.525,  0.574,  0.119])\n",
    "assert(np.array_equal(xx, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = parse_spambase_data(PATH/\"tiny.spam.train\")\n",
    "X_val, Y_val = parse_spambase_data(PATH/\"tiny.spam.test\")\n",
    "X, X_val = normalize(X, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = .1\n",
    "f0, models = boostingNN(X, Y, num_iter=10, nu=nu)\n",
    "y_hat = gradient_boosting_predict(X, f0, models, nu=nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy(Y, y_hat)\n",
    "assert(acc_train==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = gradient_boosting_predict(X_val, f0, models, nu=nu)\n",
    "acc_val = accuracy(Y_val, y_hat)\n",
    "assert(acc_val==0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = parse_spambase_data(PATH/\"spambase.train\")\n",
    "X_val, Y_val = parse_spambase_data(PATH/\"spambase.test\")\n",
    "X, X_val = normalize(X, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = .1\n",
    "f0, models = boostingNN(X, Y, num_iter=100, nu=nu)\n",
    "y_hat = gradient_boosting_predict(X, f0, models, nu=nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy(Y, y_hat)\n",
    "assert(np.around(acc_train, decimals=4)==0.9697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = gradient_boosting_predict(X_val, f0, models, nu=nu)\n",
    "acc_val = accuracy(Y_val, y_hat)\n",
    "assert(np.around(acc_val, decimals=4)==0.944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: \n",
    "\n",
    "1. Tune the boosting model to optimize accuracy on the validation set. What hyperparameters can we adjust?\n",
    "\n",
    "2. Consider adding more layers to the NN. How would this affect the need for boosting? How would it change the ideal hyperparameters? Implement a deeper NN class and do some experiments to see how this change affects the boosting hyperparameters.\n",
    "\n",
    "3. Generalize what we did with Boosting NNs for classification to Boosting NNs for regression. Consider: what changes? what remains the same? You can use the following dataset to guide your development: \n",
    "https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
