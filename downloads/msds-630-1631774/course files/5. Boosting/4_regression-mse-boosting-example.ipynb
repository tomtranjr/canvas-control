{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='table.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1160, 1200, 1280, 1450, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1418.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-258., -218., -138.,   32.,  582.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = y - y.mean()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_errors(y):\n",
    "    mu = y.mean()\n",
    "    return ((y - mu)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best stump to fit the residual\n",
    "Since we have 5 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  np.array([750, 800, 850, 900, 950])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider 4 stumps at the following points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[775.0, 825.0, 875.0, 925.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x[i] + x[i+1])/2 for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-258., -218., -138.,   32.,  582.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider different splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate sum of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-258.]), array([-218., -138.,   32.,  582.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:i], res[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 389675.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_of_errors(res[:i]), sum_of_errors(res[i:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77935.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum_of_errors(res[:i]) + sum_of_errors(res[i:]))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "[-258.] [-218. -138.   32.  582.]\n",
      "MSE: 77935.0\n",
      "\n",
      "Split 2:\n",
      "[-258. -218.] [-138.   32.  582.]\n",
      "MSE: 56813.333333333336\n",
      "\n",
      "Split 3:\n",
      "[-258. -218. -138.] [ 32. 582.]\n",
      "MSE: 31743.333333333332\n",
      "\n",
      "Split 4:\n",
      "[-258. -218. -138.   32.] [582.]\n",
      "MSE: 9895.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    print(\"Split \"+ str(i)+\":\")\n",
    "    print(res[:i], res[i:])\n",
    "    print(\"MSE: \"+ \\\n",
    "          str((sum_of_errors(res[:i]) + sum_of_errors(res[i:]))/5)+\"\\n\") \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate avg of pseudo-residuals per leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-145.5, 582.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(res[:i]), np.mean(res[i:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create next stage's tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-145.5, -145.5, -145.5, -145.5,  582. ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1 =np.array([-145.5, -145.5, -145.5, -145.5, 582])\n",
    "T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-145.5, -145.5, -145.5, -145.5,  582. ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1 = np.append(np.repeat(np.mean(res[:i]), len(res[:i])),\\\n",
    "               np.repeat(np.mean(res[i:]), len(res[i:])))\n",
    "T1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add T1 to original f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1272.5, 1272.5, 1272.5, 1272.5, 2000. ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = y.mean() +  T1\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-112.5,  -72.5,    7.5,  177.5,    0. ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y - f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizing Gradient Boosting\n",
    "## Add T1 to original f0, scaled by learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter $\\nu$ is the \"learning rate\" should satisfy $\\nu \\in (0,1]$ (cf. $\\eta$ in  gradient descent). Larger values indicate that the gradient boosting is minimizing the pseudo-residuals faster (i.e. with fewer stages) but also puts the model at risk of overfitting. Once we get to a point where the residuals in a leaf are 0, there's nothing left to learn. So then if we include that residual in another split, the next step will be trying to \"unmemorize\" that point. The balancing act of memorizing an unmemorizing is hard to understandâ€“ it's easier to understand gradual steps toward reducing the loss, so generally it's a better idea to just tune $\\nu$ so that we can slowly keep learning points better and better without worrying about having to undo our learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "nu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1272.5, 1272.5, 1272.5, 1272.5, 2000. ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = y.mean() + nu * T1\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-112.5,  -72.5,    7.5,  177.5,    0. ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y - f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hit a leaf with 0 residual after just one tree. $(M = 1.)$ Danger of overfitting. Let's slow the learning rate down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1345.25, 1345.25, 1345.25, 1345.25, 1709.  ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learning rate\n",
    "nu = 0.5\n",
    "\n",
    "f1 = y.mean() + nu * T1\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-185.25, -145.25,  -65.25,  104.75,  291.  ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y - f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice if we slow it down even further we will get even higher pseudo-residuals. That's fine if we keep building more and more trees $T_m$ to gradually reduce them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1403.45, 1403.45, 1403.45, 1403.45, 1476.2 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learning rate\n",
    "nu = 0.1\n",
    "f1 = y.mean() + nu * T1\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-243.45, -203.45, -123.45,   46.55,  523.8 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = y - f1\n",
    "res2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaway: Generally a smaller $\\nu$ will necessitate a larger # of stages $M$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit the next stage's regression tree $T_2$ using the residuals from $f_1$. \n",
    "2. What learning rate did you select? Why?\n",
    "3. With your choice of learning rate, how many trees $M$ do you think you will need in order to achieve a good fit? How would you assess this quantitatively? \n",
    "4. Repeat the gradient boosting procedure but now using MAE instead of MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "[-243.45] [-203.45 -123.45   46.55  523.8 ]\n",
      "MSE: 63669.634375\n",
      "\n",
      "Split 2:\n",
      "[-243.45 -203.45] [-123.45   46.55  523.8 ]\n",
      "MSE: 45200.00833333334\n",
      "\n",
      "Split 3:\n",
      "[-243.45 -203.45 -123.45] [ 46.55 523.8 ]\n",
      "MSE: 24270.089583333334\n",
      "\n",
      "Split 4:\n",
      "[-243.45 -203.45 -123.45   46.55] [523.8]\n",
      "MSE: 9895.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    print(\"Split \"+ str(i)+\":\")\n",
    "    print(res2[:i], res2[i:])\n",
    "    print(\"MSE: \"+ \\\n",
    "          str((sum_of_errors(res2[:i]) + sum_of_errors(res2[i:]))/5)+\"\\n\") \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-190.11666667, -190.11666667, -190.11666667,  285.175     ,\n",
       "        285.175     ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T2 = np.append(np.repeat(np.mean(res2[:i]), len(res2[:i])),\\\n",
    "               np.repeat(np.mean(res2[i:]), len(res2[i:])))\n",
    "T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = f1 + nu * T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1384.43833333, 1384.43833333, 1384.43833333, 1431.9675    ,\n",
       "       1504.7175    ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-224.43833333, -184.43833333, -104.43833333,   18.0325    ,\n",
       "        495.2825    ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3 = y - f2\n",
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
